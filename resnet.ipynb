{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjJ6qg-UfzsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "18c00cfc-b8c3-40b0-8775-2ff946ff6784"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "\n",
        "!mkdir -p drive\n",
        "\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131331 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.6-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUualJAmf1_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "d5ecb7c7-95db-46ef-9daa-991696b0fa39"
      },
      "source": [
        "!unzip -q drive/images.zip\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  drive  images  sample_data  test.txt\ttrain.txt  val.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkkf7ylDf498",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average\n",
        "import numpy as np\n",
        "import os, pdb\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "import threading\n",
        "import time\n",
        "\n",
        "global n_classes, ema_gp\n",
        "ema_gp = []\n",
        "n_classes = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-3LtwwCf62k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(x,name=\"activation\"):\n",
        "    return tf.nn.relu(x, name=name)\n",
        "    \n",
        "def conv2d(name, l_input, w, b, s, p):\n",
        "    l_input = tf.nn.conv2d(l_input, w, strides=[1,s,s,1], padding=p, name=name)\n",
        "    l_input = l_input+b\n",
        "\n",
        "    return l_input\n",
        "\n",
        "def batchnorm(conv, isTraining, name='bn'):\n",
        "    return tf.layers.batch_normalization(conv, training=isTraining, name=\"bn\"+name)\n",
        "\n",
        "def initializer(in_filters, out_filters, name):\n",
        "    w1 = tf.get_variable(name+\"W\", [3, 3, in_filters, out_filters], initializer=tf.truncated_normal_initializer())\n",
        "    b1 = tf.get_variable(name+\"B\", [out_filters], initializer=tf.truncated_normal_initializer())\n",
        "    return w1, b1\n",
        "  \n",
        "def residual_block(in_x, in_filters, out_filters, stride, isDownSampled, name, isTraining):\n",
        "    global ema_gp\n",
        "    # first convolution layer\n",
        "    if isDownSampled:\n",
        "      in_x = tf.nn.avg_pool(in_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "      \n",
        "    x = batchnorm(in_x, isTraining, name=name+'FirstBn')\n",
        "    x = activation(x)\n",
        "    w1, b1 = initializer(in_filters, in_filters, name+\"first_res\")\n",
        "    x = conv2d(name+'r1', x, w1, b1, 1, \"SAME\")\n",
        "\n",
        "    # second convolution layer\n",
        "    x = batchnorm(x, isTraining, name=name+'SecondBn')\n",
        "    x = activation(x)\n",
        "    w2, b2 = initializer(in_filters, out_filters, name+\"Second_res\")\n",
        "    x = conv2d(name+'r2', x, w2, b2, 1, \"SAME\")\n",
        "    \n",
        "    if in_filters != out_filters:\n",
        "        difference = out_filters - in_filters\n",
        "        left_pad = difference // 2\n",
        "        right_pad = difference - left_pad\n",
        "        identity = tf.pad(in_x, [[0, 0], [0, 0], [0, 0], [left_pad, right_pad]])\n",
        "        return x + identity\n",
        "    else:\n",
        "        return in_x + x\n",
        "\n",
        "      \n",
        "def ResNet(_X, isTraining):\n",
        "    global n_classes\n",
        "    w1 = tf.get_variable(\"initW\", [7, 7, 3, 96], initializer=tf.truncated_normal_initializer())\n",
        "    b1 = tf.get_variable(\"initB\", [96], initializer=tf.truncated_normal_initializer())\n",
        "    x = conv2d('conv1', _X, w1, b1, 4, \"VALID\")\n",
        "    \n",
        "    filters_num = [96,128,256,384]\n",
        "    block_num = [2,2,2,2]\n",
        "    l_cnt = 1\n",
        "    for i in range(len(filters_num)):\n",
        "      for j in range(block_num[i]):\n",
        "          \n",
        "          if ((j==block_num[i]-1) & (i<len(filters_num)-1)):\n",
        "            x = residual_block(x, filters_num[i], filters_num[i+1], 2, True, 'ResidualBlock%d'%(l_cnt), isTraining)\n",
        "            print('[L-%d] Build %dth connection layer %d from %d to %d channels' % (l_cnt, i, j, filters_num[i], filters_num[i+1]))\n",
        "          else:\n",
        "            x = residual_block(x, filters_num[i], filters_num[i], 1, False, 'ResidualBlock%d'%(l_cnt), isTraining)\n",
        "            print('[L-%d] Build %dth residual block %d with %d channels' % (l_cnt,i, j, filters_num[i]))\n",
        "          l_cnt +=1\n",
        "\n",
        "    \n",
        "    wo, bo=initializer(filters_num[-1], n_classes, \"FinalOutput\")\n",
        "    x = conv2d('final', x, wo, bo, 1, \"SAME\")\n",
        "    \n",
        "    x = activation(x)\n",
        "    x = batchnorm(x, isTraining, name='FinalBn')\n",
        "    \n",
        "    x = tf.reduce_mean(x, [1,2])\n",
        "    W = tf.get_variable(\"FinalW\", [n_classes, n_classes], initializer=tf.truncated_normal_initializer())\n",
        "    b = tf.get_variable(\"FinalB\", [n_classes], initializer=tf.truncated_normal_initializer())\n",
        "    \n",
        "    out = tf.matmul(x, W) + b\n",
        "                            \n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPVhoha-f-is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==========================================================================\n",
        "#=============Reading data in multithreading manner========================\n",
        "#==========================================================================\n",
        "def read_labeled_image_list(image_list_file, training_img_dir):\n",
        "    \"\"\"Reads a .txt file containing pathes and labeles\n",
        "    Args:\n",
        "       image_list_file: a .txt file with one /path/to/image per line\n",
        "       label: optionally, if set label will be pasted after each line\n",
        "    Returns:\n",
        "       List with all filenames in file image_list_file\n",
        "    \"\"\"\n",
        "    f = open(image_list_file, 'r')\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    for line in f:\n",
        "        filename, label = line[:-1].split(' ')\n",
        "        filename = training_img_dir+filename\n",
        "        filenames.append(filename)\n",
        "        labels.append(int(label))\n",
        "        \n",
        "    return filenames, labels\n",
        "    \n",
        "    \n",
        "def read_images_from_disk(input_queue, size1=256):\n",
        "    \"\"\"Consumes a single filename and label as a ' '-delimited string.\n",
        "    Args:\n",
        "      filename_and_label_tensor: A scalar string tensor.\n",
        "    Returns:\n",
        "      Two tensors: the decoded image, and the string label.\n",
        "    \"\"\"\n",
        "    label = input_queue[1]\n",
        "    fn=input_queue[0]\n",
        "    file_contents = tf.read_file(input_queue[0])\n",
        "    example = tf.image.decode_jpeg(file_contents, channels=3)\n",
        "    \n",
        "    #example = tf.image.decode_png(file_contents, channels=3, name=\"dataset_image\") # png fo rlfw\n",
        "    example=tf.image.resize_images(example, [size1,size1])\n",
        "    return example, label, fn\n",
        "    \n",
        "def setup_inputs(sess, filenames, training_img_dir, image_size=256, crop_size=224, isTest=False, batch_size=128):\n",
        "    \n",
        "    # Read each image file\n",
        "    image_list, label_list = read_labeled_image_list(filenames, training_img_dir)\n",
        "\n",
        "    images = tf.cast(image_list, tf.string)\n",
        "    labels = tf.cast(label_list, tf.int64)\n",
        "     # Makes an input queue\n",
        "    if isTest is False:\n",
        "        isShuffle = True\n",
        "        numThr = 4\n",
        "    else:\n",
        "        isShuffle = False\n",
        "        numThr = 1\n",
        "        \n",
        "    input_queue = tf.train.slice_input_producer([images, labels], shuffle=isShuffle)\n",
        "    image, y,fn = read_images_from_disk(input_queue)\n",
        "\n",
        "    channels = 3\n",
        "    image.set_shape([None, None, channels])\n",
        "        \n",
        "    # Crop and other random augmentations\n",
        "    if isTest is False:\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_saturation(image, .95, 1.05)\n",
        "        image = tf.image.random_brightness(image, .05)\n",
        "        image = tf.image.random_contrast(image, .95, 1.05)\n",
        "        \n",
        "\n",
        "    image = tf.random_crop(image, [crop_size, crop_size, 3])\n",
        "    image = tf.cast(image, tf.float32)/255.0\n",
        "    \n",
        "    image, y,fn = tf.train.batch([image, y, fn], batch_size=batch_size, capacity=batch_size*3, num_threads=numThr, name='labels_and_images')\n",
        "\n",
        "    tf.train.start_queue_runners(sess=sess)\n",
        "\n",
        "    return image, y, fn, len(label_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJxxZXeTgBKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "cdd5838c-c1a9-4c14-b19e-8fbe87e168e8"
      },
      "source": [
        "batch_size = 256\n",
        "display_step = 80\n",
        "learning_rate = tf.placeholder(tf.float32)      # Learning rate to be fed\n",
        "lr = 1e-2                          # Learning rate start\n",
        "print('GO!!')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GO!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOx58zm8gC8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fed4991a-0cc0-4da8-ee10-11523e1e7514"
      },
      "source": [
        "# Setup the tensorflow...\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "print(\"Preparing the training & validation data...\")\n",
        "train_data, train_labels, filelist1, glen1 = setup_inputs(sess, \"train.txt\", \"./\", batch_size=batch_size)\n",
        "val_data, val_labels, filelist2, tlen1 = setup_inputs(sess, \"val.txt\", \"./\", batch_size=batch_size,isTest=True)\n",
        "\n",
        "\n",
        "max_iter = glen1*100\n",
        "print(\"Preparing the training model with learning rate = %.5f...\" % (lr))\n",
        "\n",
        "\n",
        "with tf.variable_scope(\"ResNet\") as scope:\n",
        "  pred = ResNet(train_data, True)\n",
        "  scope.reuse_variables()\n",
        "  valpred = ResNet(val_data, False)\n",
        "\n",
        "with tf.name_scope('Loss_and_Accuracy'):\n",
        "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "  with tf.control_dependencies(update_ops):\n",
        "    cost = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=pred)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "    \n",
        "  correct_prediction = tf.equal(tf.argmax(pred, 1), train_labels)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  top5=tf.reduce_mean(tf.cast(tf.nn.in_top_k(pred, train_labels, 5), tf.float32))\n",
        "  \n",
        "  correct_prediction2 = tf.equal(tf.argmax(valpred, 1), val_labels)\n",
        "  accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n",
        "  \n",
        "  tf.summary.scalar('Loss', cost)\n",
        "  tf.summary.scalar('Training_Accuracy', accuracy)\n",
        "  tf.summary.scalar('Top-5_accuracy', top5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing the training & validation data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0730 10:45:51.262284 140396069877632 deprecation.py:323] From <ipython-input-5-fbefbd28a462>:53: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0730 10:45:51.269682 140396069877632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:374: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0730 10:45:51.291357 140396069877632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:320: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0730 10:45:51.293069 140396069877632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "W0730 10:45:51.298562 140396069877632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0730 10:45:51.301117 140396069877632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0730 10:45:51.353453 140396069877632 deprecation.py:323] From <ipython-input-5-fbefbd28a462>:70: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0730 10:45:51.364784 140396069877632 deprecation.py:323] From <ipython-input-5-fbefbd28a462>:72: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preparing the training model with learning rate = 0.01000...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0730 10:45:52.835245 140396069877632 deprecation.py:323] From <ipython-input-4-0a28748977c8>:11: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[L-1] Build 0th residual block 0 with 96 channels\n",
            "[L-2] Build 0th connection layer 1 from 96 to 128 channels\n",
            "[L-3] Build 1th residual block 0 with 128 channels\n",
            "[L-4] Build 1th connection layer 1 from 128 to 256 channels\n",
            "[L-5] Build 2th residual block 0 with 256 channels\n",
            "[L-6] Build 2th connection layer 1 from 256 to 384 channels\n",
            "[L-7] Build 3th residual block 0 with 384 channels\n",
            "[L-8] Build 3th residual block 1 with 384 channels\n",
            "[L-1] Build 0th residual block 0 with 96 channels\n",
            "[L-2] Build 0th connection layer 1 from 96 to 128 channels\n",
            "[L-3] Build 1th residual block 0 with 128 channels\n",
            "[L-4] Build 1th connection layer 1 from 128 to 256 channels"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0730 10:45:54.379867 140396069877632 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[L-5] Build 2th residual block 0 with 256 channels\n",
            "[L-6] Build 2th connection layer 1 from 256 to 384 channels\n",
            "[L-7] Build 3th residual block 0 with 384 channels\n",
            "[L-8] Build 3th residual block 1 with 384 channels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZfqwM_cgIk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97565457-b7ab-4650-de90-88c85ce0b4ef"
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "step = 0\n",
        "writer = tf.summary.FileWriter(\"log2\", sess.graph)\n",
        "summaries = tf.summary.merge_all()\n",
        "\n",
        "print(\"We are going to train the ImageNet model based on ResNet!!!\")\n",
        "while (step * batch_size) < max_iter:\n",
        "    epoch1=np.floor((step*batch_size)/glen1)\n",
        "    if (((step*batch_size)%glen1 < batch_size) & (lr==1e-3) & (epoch1 >2)):\n",
        "        lr /= 10\n",
        "\n",
        "    sess.run(optimizer,  feed_dict={learning_rate: lr})\n",
        "\n",
        "    if (step % 15000==1) & (step>15000):\n",
        "        save_path = saver.save(sess, \"./tf_resnet_model_iter\" + str(step) + \".ckpt\")\n",
        "        print(\"Model saved in file at iteration %d: %s\" % (step*batch_size,save_path))\n",
        "\n",
        "    if step>1 and step % display_step == 1:\n",
        "        # calculate the loss\n",
        "        loss, acc, top5acc, summaries_string = sess.run([cost, accuracy,top5, summaries])\n",
        "        print(\"Iter=%d/epoch=%d, Loss=%.6f, Training Accuracy=%.6f, Top-5 Accuracy=%.6f, lr=%f\" % (step*batch_size, epoch1 ,loss, acc, top5acc, lr))\n",
        "        writer.add_summary(summaries_string, step)\n",
        "        \n",
        "    if step>1 and (step % (display_step*10) == 1):\n",
        "        rounds = tlen1 // batch_size\n",
        "        valacc=[]\n",
        "        for k in range(rounds):\n",
        "          a2 = sess.run(accuracy2)\n",
        "          valacc.append(a2)\n",
        "        print(\"\\nIter=%d/epoch=%d, Validation Accuracy=%.6f\" % (step*batch_size, epoch1 , np.mean(valacc)))\n",
        "\n",
        "  \n",
        "    step += 1\n",
        "print(\"Optimization Finished!\")\n",
        "save_path = saver.save(sess, \"./tf_resnet_model.ckpt\")\n",
        "print(\"Model saved in file: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are going to train the ImageNet model based on ResNet!!!\n",
            "Iter=20736/epoch=0, Loss=3.175990, Training Accuracy=0.152344, Top-5 Accuracy=0.480469, lr=0.010000\n",
            "Iter=41216/epoch=0, Loss=3.028897, Training Accuracy=0.238281, Top-5 Accuracy=0.503906, lr=0.010000\n",
            "Iter=61696/epoch=0, Loss=2.789783, Training Accuracy=0.261719, Top-5 Accuracy=0.585938, lr=0.010000\n",
            "Iter=82176/epoch=1, Loss=2.572429, Training Accuracy=0.289062, Top-5 Accuracy=0.636719, lr=0.010000\n",
            "Iter=102656/epoch=1, Loss=2.382948, Training Accuracy=0.332031, Top-5 Accuracy=0.695312, lr=0.010000\n",
            "Iter=123136/epoch=1, Loss=2.321332, Training Accuracy=0.343750, Top-5 Accuracy=0.699219, lr=0.010000\n",
            "Iter=143616/epoch=2, Loss=2.136219, Training Accuracy=0.375000, Top-5 Accuracy=0.753906, lr=0.010000\n",
            "Iter=164096/epoch=2, Loss=1.931998, Training Accuracy=0.414062, Top-5 Accuracy=0.792969, lr=0.010000\n",
            "Iter=184576/epoch=2, Loss=1.937448, Training Accuracy=0.429688, Top-5 Accuracy=0.785156, lr=0.010000\n",
            "Iter=205056/epoch=3, Loss=1.791667, Training Accuracy=0.421875, Top-5 Accuracy=0.832031, lr=0.010000\n",
            "\n",
            "Iter=205056/epoch=3, Validation Accuracy=0.167969\n",
            "Iter=225536/epoch=3, Loss=1.694357, Training Accuracy=0.492188, Top-5 Accuracy=0.832031, lr=0.010000\n",
            "Iter=246016/epoch=3, Loss=1.790664, Training Accuracy=0.472656, Top-5 Accuracy=0.847656, lr=0.010000\n",
            "Iter=266496/epoch=4, Loss=1.632793, Training Accuracy=0.468750, Top-5 Accuracy=0.847656, lr=0.010000\n",
            "Iter=286976/epoch=4, Loss=1.592443, Training Accuracy=0.507812, Top-5 Accuracy=0.855469, lr=0.010000\n",
            "Iter=307456/epoch=4, Loss=1.609158, Training Accuracy=0.500000, Top-5 Accuracy=0.839844, lr=0.010000\n",
            "Iter=327936/epoch=5, Loss=1.444284, Training Accuracy=0.554688, Top-5 Accuracy=0.863281, lr=0.010000\n",
            "Iter=348416/epoch=5, Loss=1.301650, Training Accuracy=0.617188, Top-5 Accuracy=0.898438, lr=0.010000\n",
            "Iter=368896/epoch=5, Loss=1.414291, Training Accuracy=0.542969, Top-5 Accuracy=0.882812, lr=0.010000\n",
            "Iter=389376/epoch=6, Loss=1.376902, Training Accuracy=0.535156, Top-5 Accuracy=0.890625, lr=0.010000\n",
            "Iter=409856/epoch=6, Loss=1.514058, Training Accuracy=0.546875, Top-5 Accuracy=0.910156, lr=0.010000\n",
            "\n",
            "Iter=409856/epoch=6, Validation Accuracy=0.542969\n",
            "Iter=430336/epoch=6, Loss=1.232593, Training Accuracy=0.597656, Top-5 Accuracy=0.886719, lr=0.010000\n",
            "Iter=450816/epoch=7, Loss=1.139678, Training Accuracy=0.656250, Top-5 Accuracy=0.925781, lr=0.010000\n",
            "Iter=471296/epoch=7, Loss=1.232186, Training Accuracy=0.597656, Top-5 Accuracy=0.917969, lr=0.010000\n",
            "Iter=491776/epoch=7, Loss=1.315674, Training Accuracy=0.609375, Top-5 Accuracy=0.910156, lr=0.010000\n",
            "Iter=512256/epoch=8, Loss=1.291614, Training Accuracy=0.585938, Top-5 Accuracy=0.910156, lr=0.010000\n",
            "Iter=532736/epoch=8, Loss=1.000082, Training Accuracy=0.664062, Top-5 Accuracy=0.941406, lr=0.010000\n",
            "Iter=553216/epoch=8, Loss=1.160084, Training Accuracy=0.648438, Top-5 Accuracy=0.917969, lr=0.010000\n",
            "Iter=573696/epoch=9, Loss=1.018677, Training Accuracy=0.683594, Top-5 Accuracy=0.937500, lr=0.010000\n",
            "Iter=594176/epoch=9, Loss=1.047072, Training Accuracy=0.683594, Top-5 Accuracy=0.929688, lr=0.010000\n",
            "Iter=614656/epoch=9, Loss=1.274448, Training Accuracy=0.628906, Top-5 Accuracy=0.902344, lr=0.010000\n",
            "\n",
            "Iter=614656/epoch=9, Validation Accuracy=0.406250\n",
            "Iter=635136/epoch=10, Loss=1.025892, Training Accuracy=0.707031, Top-5 Accuracy=0.910156, lr=0.010000\n",
            "Iter=655616/epoch=10, Loss=0.978589, Training Accuracy=0.691406, Top-5 Accuracy=0.945312, lr=0.010000\n",
            "Iter=676096/epoch=10, Loss=1.036223, Training Accuracy=0.656250, Top-5 Accuracy=0.925781, lr=0.010000\n",
            "Iter=696576/epoch=11, Loss=0.861984, Training Accuracy=0.757812, Top-5 Accuracy=0.941406, lr=0.010000\n",
            "Iter=717056/epoch=11, Loss=0.912031, Training Accuracy=0.746094, Top-5 Accuracy=0.949219, lr=0.010000\n",
            "Iter=737536/epoch=11, Loss=0.849066, Training Accuracy=0.722656, Top-5 Accuracy=0.941406, lr=0.010000\n",
            "Iter=758016/epoch=11, Loss=0.626477, Training Accuracy=0.828125, Top-5 Accuracy=0.980469, lr=0.010000\n",
            "Iter=778496/epoch=12, Loss=0.844373, Training Accuracy=0.734375, Top-5 Accuracy=0.964844, lr=0.010000\n",
            "Iter=798976/epoch=12, Loss=0.891308, Training Accuracy=0.703125, Top-5 Accuracy=0.960938, lr=0.010000\n",
            "Iter=819456/epoch=12, Loss=0.721122, Training Accuracy=0.765625, Top-5 Accuracy=0.964844, lr=0.010000\n",
            "\n",
            "Iter=819456/epoch=12, Validation Accuracy=0.292969\n",
            "Iter=839936/epoch=13, Loss=0.755147, Training Accuracy=0.750000, Top-5 Accuracy=0.968750, lr=0.010000\n",
            "Iter=860416/epoch=13, Loss=0.760461, Training Accuracy=0.777344, Top-5 Accuracy=0.960938, lr=0.010000\n",
            "Iter=880896/epoch=13, Loss=0.653655, Training Accuracy=0.796875, Top-5 Accuracy=0.976562, lr=0.010000\n",
            "Iter=901376/epoch=14, Loss=0.761584, Training Accuracy=0.742188, Top-5 Accuracy=0.968750, lr=0.010000\n",
            "Iter=921856/epoch=14, Loss=0.729757, Training Accuracy=0.750000, Top-5 Accuracy=0.968750, lr=0.010000\n",
            "Iter=942336/epoch=14, Loss=0.579287, Training Accuracy=0.796875, Top-5 Accuracy=0.980469, lr=0.010000\n",
            "Iter=962816/epoch=15, Loss=0.646375, Training Accuracy=0.781250, Top-5 Accuracy=0.980469, lr=0.010000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_bXU3arwZcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIEjKdMfgJF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exit()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}